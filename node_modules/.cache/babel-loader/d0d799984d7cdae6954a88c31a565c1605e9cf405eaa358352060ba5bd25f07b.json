{"ast":null,"code":"import _objectSpread from \"/Users/apple/Documents/treasure/node_modules/@babel/runtime/helpers/esm/objectSpread2.js\";\nimport _asyncToGenerator from \"/Users/apple/Documents/treasure/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _regeneratorRuntime from \"/Users/apple/Documents/treasure/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _awaitAsyncGenerator from \"/Users/apple/Documents/treasure/node_modules/@babel/runtime/helpers/esm/awaitAsyncGenerator.js\";\nimport _wrapAsyncGenerator from \"/Users/apple/Documents/treasure/node_modules/@babel/runtime/helpers/esm/wrapAsyncGenerator.js\";\nimport _asyncIterator from \"/Users/apple/Documents/treasure/node_modules/@babel/runtime/helpers/esm/asyncIterator.js\";\nimport errCode from 'err-code';\nimport { UnixFS } from 'ipfs-unixfs';\nimport persist from '../../utils/persist.js';\nimport { encode, prepare } from '@ipld/dag-pb';\nimport parallelBatch from 'it-parallel-batch';\nimport * as rawCodec from 'multiformats/codecs/raw';\nimport * as dagPb from '@ipld/dag-pb';\nimport dagFlat from './flat.js';\nimport dagBalanced from './balanced.js';\nimport dagTrickle from './trickle.js';\nimport bufferImporterFn from './buffer-importer.js';\n\n/**\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\nvar dagBuilders = {\n  flat: dagFlat,\n  balanced: dagBalanced,\n  trickle: dagTrickle\n};\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nfunction buildFileBatch(_x, _x2, _x3) {\n  return _buildFileBatch.apply(this, arguments);\n}\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nfunction _buildFileBatch() {\n  _buildFileBatch = _wrapAsyncGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(file, blockstore, options) {\n    var count, previous, bufferImporter, _iteratorAbruptCompletion, _didIteratorError, _iteratorError, _iterator, _step, entry;\n    return _regeneratorRuntime().wrap(function _callee$(_context) {\n      while (1) switch (_context.prev = _context.next) {\n        case 0:\n          count = -1;\n          if (typeof options.bufferImporter === 'function') {\n            bufferImporter = options.bufferImporter;\n          } else {\n            bufferImporter = bufferImporterFn;\n          }\n          _iteratorAbruptCompletion = false;\n          _didIteratorError = false;\n          _context.prev = 4;\n          _iterator = _asyncIterator(parallelBatch(bufferImporter(file, blockstore, options), options.blockWriteConcurrency));\n        case 6:\n          _context.next = 8;\n          return _awaitAsyncGenerator(_iterator.next());\n        case 8:\n          if (!(_iteratorAbruptCompletion = !(_step = _context.sent).done)) {\n            _context.next = 25;\n            break;\n          }\n          entry = _step.value;\n          count++;\n          if (!(count === 0)) {\n            _context.next = 16;\n            break;\n          }\n          previous = entry;\n          return _context.abrupt(\"continue\", 22);\n        case 16:\n          if (!(count === 1 && previous)) {\n            _context.next = 20;\n            break;\n          }\n          _context.next = 19;\n          return previous;\n        case 19:\n          previous = null;\n        case 20:\n          _context.next = 22;\n          return entry;\n        case 22:\n          _iteratorAbruptCompletion = false;\n          _context.next = 6;\n          break;\n        case 25:\n          _context.next = 31;\n          break;\n        case 27:\n          _context.prev = 27;\n          _context.t0 = _context[\"catch\"](4);\n          _didIteratorError = true;\n          _iteratorError = _context.t0;\n        case 31:\n          _context.prev = 31;\n          _context.prev = 32;\n          if (!(_iteratorAbruptCompletion && _iterator.return != null)) {\n            _context.next = 36;\n            break;\n          }\n          _context.next = 36;\n          return _awaitAsyncGenerator(_iterator.return());\n        case 36:\n          _context.prev = 36;\n          if (!_didIteratorError) {\n            _context.next = 39;\n            break;\n          }\n          throw _iteratorError;\n        case 39:\n          return _context.finish(36);\n        case 40:\n          return _context.finish(31);\n        case 41:\n          if (!previous) {\n            _context.next = 45;\n            break;\n          }\n          previous.single = true;\n          _context.next = 45;\n          return previous;\n        case 45:\n        case \"end\":\n          return _context.stop();\n      }\n    }, _callee, null, [[4, 27, 31, 41], [32,, 36, 40]]);\n  }));\n  return _buildFileBatch.apply(this, arguments);\n}\nvar reduce = function reduce(file, blockstore, options) {\n  /**\n   * @type {Reducer}\n   */\n  function reducer(_x4) {\n    return _reducer.apply(this, arguments);\n  }\n  function _reducer() {\n    _reducer = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(leaves) {\n      var leaf, _buffer, f, links, node, buffer, cid;\n      return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n        while (1) switch (_context2.prev = _context2.next) {\n          case 0:\n            if (!(leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf)) {\n              _context2.next = 13;\n              break;\n            }\n            leaf = leaves[0];\n            if (!(file.mtime !== undefined || file.mode !== undefined)) {\n              _context2.next = 12;\n              break;\n            }\n            _context2.next = 5;\n            return blockstore.get(leaf.cid);\n          case 5:\n            _buffer = _context2.sent;\n            leaf.unixfs = new UnixFS({\n              type: 'file',\n              mtime: file.mtime,\n              mode: file.mode,\n              data: _buffer\n            });\n            _buffer = encode(prepare({\n              Data: leaf.unixfs.marshal()\n            }));\n\n            // // TODO vmx 2021-03-26: This is what the original code does, it checks\n            // // the multihash of the original leaf node and uses then the same\n            // // hasher. i wonder if that's really needed or if we could just use\n            // // the hasher from `options.hasher` instead.\n            // const multihash = mh.decode(leaf.cid.multihash.bytes)\n            // let hasher\n            // switch multihash {\n            //   case sha256.code {\n            //     hasher = sha256\n            //     break;\n            //   }\n            //   //case identity.code {\n            //   //  hasher = identity\n            //   //  break;\n            //   //}\n            //   default: {\n            //     throw new Error(`Unsupported hasher \"${multihash}\"`)\n            //   }\n            // }\n            _context2.next = 10;\n            return persist(_buffer, blockstore, _objectSpread(_objectSpread({}, options), {}, {\n              codec: dagPb,\n              hasher: options.hasher,\n              cidVersion: options.cidVersion\n            }));\n          case 10:\n            leaf.cid = _context2.sent;\n            leaf.size = _buffer.length;\n          case 12:\n            return _context2.abrupt(\"return\", {\n              cid: leaf.cid,\n              path: file.path,\n              unixfs: leaf.unixfs,\n              size: leaf.size\n            });\n          case 13:\n            // create a parent node and add all the leaves\n            f = new UnixFS({\n              type: 'file',\n              mtime: file.mtime,\n              mode: file.mode\n            });\n            links = leaves.filter(function (leaf) {\n              if (leaf.cid.code === rawCodec.code && leaf.size) {\n                return true;\n              }\n              if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n                return true;\n              }\n              return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length);\n            }).map(function (leaf) {\n              if (leaf.cid.code === rawCodec.code) {\n                // node is a leaf buffer\n                f.addBlockSize(leaf.size);\n                return {\n                  Name: '',\n                  Tsize: leaf.size,\n                  Hash: leaf.cid\n                };\n              }\n              if (!leaf.unixfs || !leaf.unixfs.data) {\n                // node is an intermediate node\n                f.addBlockSize(leaf.unixfs && leaf.unixfs.fileSize() || 0);\n              } else {\n                // node is a unixfs 'file' leaf node\n                f.addBlockSize(leaf.unixfs.data.length);\n              }\n              return {\n                Name: '',\n                Tsize: leaf.size,\n                Hash: leaf.cid\n              };\n            });\n            node = {\n              Data: f.marshal(),\n              Links: links\n            };\n            buffer = encode(prepare(node));\n            _context2.next = 19;\n            return persist(buffer, blockstore, options);\n          case 19:\n            cid = _context2.sent;\n            return _context2.abrupt(\"return\", {\n              cid: cid,\n              path: file.path,\n              unixfs: f,\n              size: buffer.length + node.Links.reduce(function (acc, curr) {\n                return acc + curr.Tsize;\n              }, 0)\n            });\n          case 21:\n          case \"end\":\n            return _context2.stop();\n        }\n      }, _callee2);\n    }));\n    return _reducer.apply(this, arguments);\n  }\n  return reducer;\n};\n\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\nfunction fileBuilder(file, block, options) {\n  var dagBuilder = dagBuilders[options.strategy];\n  if (!dagBuilder) {\n    throw errCode(new Error(\"Unknown importer build strategy name: \".concat(options.strategy)), 'ERR_BAD_STRATEGY');\n  }\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options);\n}\nexport default fileBuilder;","map":{"version":3,"names":["errCode","UnixFS","persist","encode","prepare","parallelBatch","rawCodec","dagPb","dagFlat","dagBalanced","dagTrickle","bufferImporterFn","dagBuilders","flat","balanced","trickle","buildFileBatch","_x","_x2","_x3","_buildFileBatch","apply","arguments","_wrapAsyncGenerator","_regeneratorRuntime","mark","_callee","file","blockstore","options","count","previous","bufferImporter","_iteratorAbruptCompletion","_didIteratorError","_iteratorError","_iterator","_step","entry","wrap","_callee$","_context","prev","next","_asyncIterator","blockWriteConcurrency","_awaitAsyncGenerator","sent","done","value","abrupt","t0","return","finish","single","stop","reduce","reducer","_x4","_reducer","_asyncToGenerator","_callee2","leaves","leaf","_buffer","f","links","node","buffer","cid","_callee2$","_context2","length","reduceSingleLeafToSelf","mtime","undefined","mode","get","unixfs","type","data","Data","marshal","_objectSpread","codec","hasher","cidVersion","size","path","filter","code","fileSize","Boolean","map","addBlockSize","Name","Tsize","Hash","Links","acc","curr","fileBuilder","block","dagBuilder","strategy","Error","concat"],"sources":["/Users/apple/Documents/treasure/node_modules/ipfs-unixfs-importer/src/dag-builder/file/index.js"],"sourcesContent":["import errCode from 'err-code'\nimport { UnixFS } from 'ipfs-unixfs'\nimport persist from '../../utils/persist.js'\nimport { encode, prepare } from '@ipld/dag-pb'\nimport parallelBatch from 'it-parallel-batch'\nimport * as rawCodec from 'multiformats/codecs/raw'\nimport * as dagPb from '@ipld/dag-pb'\n\nimport dagFlat from './flat.js'\nimport dagBalanced from './balanced.js'\nimport dagTrickle from './trickle.js'\nimport bufferImporterFn from './buffer-importer.js'\n\n/**\n * @typedef {import('interface-blockstore').Blockstore} Blockstore\n * @typedef {import('../../types').File} File\n * @typedef {import('../../types').ImporterOptions} ImporterOptions\n * @typedef {import('../../types').Reducer} Reducer\n * @typedef {import('../../types').DAGBuilder} DAGBuilder\n * @typedef {import('../../types').FileDAGBuilder} FileDAGBuilder\n */\n\n/**\n * @type {{ [key: string]: FileDAGBuilder}}\n */\nconst dagBuilders = {\n  flat: dagFlat,\n  balanced: dagBalanced,\n  trickle: dagTrickle\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nasync function * buildFileBatch (file, blockstore, options) {\n  let count = -1\n  let previous\n  let bufferImporter\n\n  if (typeof options.bufferImporter === 'function') {\n    bufferImporter = options.bufferImporter\n  } else {\n    bufferImporter = bufferImporterFn\n  }\n\n  for await (const entry of parallelBatch(bufferImporter(file, blockstore, options), options.blockWriteConcurrency)) {\n    count++\n\n    if (count === 0) {\n      previous = entry\n      continue\n    } else if (count === 1 && previous) {\n      yield previous\n      previous = null\n    }\n\n    yield entry\n  }\n\n  if (previous) {\n    previous.single = true\n    yield previous\n  }\n}\n\n/**\n * @param {File} file\n * @param {Blockstore} blockstore\n * @param {ImporterOptions} options\n */\nconst reduce = (file, blockstore, options) => {\n  /**\n   * @type {Reducer}\n   */\n  async function reducer (leaves) {\n    if (leaves.length === 1 && leaves[0].single && options.reduceSingleLeafToSelf) {\n      const leaf = leaves[0]\n\n      if (file.mtime !== undefined || file.mode !== undefined) {\n        // only one leaf node which is a buffer - we have metadata so convert it into a\n        // UnixFS entry otherwise we'll have nowhere to store the metadata\n        let buffer = await blockstore.get(leaf.cid)\n\n        leaf.unixfs = new UnixFS({\n          type: 'file',\n          mtime: file.mtime,\n          mode: file.mode,\n          data: buffer\n        })\n\n        buffer = encode(prepare({ Data: leaf.unixfs.marshal() }))\n\n        // // TODO vmx 2021-03-26: This is what the original code does, it checks\n        // // the multihash of the original leaf node and uses then the same\n        // // hasher. i wonder if that's really needed or if we could just use\n        // // the hasher from `options.hasher` instead.\n        // const multihash = mh.decode(leaf.cid.multihash.bytes)\n        // let hasher\n        // switch multihash {\n        //   case sha256.code {\n        //     hasher = sha256\n        //     break;\n        //   }\n        //   //case identity.code {\n        //   //  hasher = identity\n        //   //  break;\n        //   //}\n        //   default: {\n        //     throw new Error(`Unsupported hasher \"${multihash}\"`)\n        //   }\n        // }\n        leaf.cid = await persist(buffer, blockstore, {\n          ...options,\n          codec: dagPb,\n          hasher: options.hasher,\n          cidVersion: options.cidVersion\n        })\n        leaf.size = buffer.length\n      }\n\n      return {\n        cid: leaf.cid,\n        path: file.path,\n        unixfs: leaf.unixfs,\n        size: leaf.size\n      }\n    }\n\n    // create a parent node and add all the leaves\n    const f = new UnixFS({\n      type: 'file',\n      mtime: file.mtime,\n      mode: file.mode\n    })\n\n    const links = leaves\n      .filter(leaf => {\n        if (leaf.cid.code === rawCodec.code && leaf.size) {\n          return true\n        }\n\n        if (leaf.unixfs && !leaf.unixfs.data && leaf.unixfs.fileSize()) {\n          return true\n        }\n\n        return Boolean(leaf.unixfs && leaf.unixfs.data && leaf.unixfs.data.length)\n      })\n      .map((leaf) => {\n        if (leaf.cid.code === rawCodec.code) {\n          // node is a leaf buffer\n          f.addBlockSize(leaf.size)\n\n          return {\n            Name: '',\n            Tsize: leaf.size,\n            Hash: leaf.cid\n          }\n        }\n\n        if (!leaf.unixfs || !leaf.unixfs.data) {\n          // node is an intermediate node\n          f.addBlockSize((leaf.unixfs && leaf.unixfs.fileSize()) || 0)\n        } else {\n          // node is a unixfs 'file' leaf node\n          f.addBlockSize(leaf.unixfs.data.length)\n        }\n\n        return {\n          Name: '',\n          Tsize: leaf.size,\n          Hash: leaf.cid\n        }\n      })\n\n    const node = {\n      Data: f.marshal(),\n      Links: links\n    }\n    const buffer = encode(prepare(node))\n    const cid = await persist(buffer, blockstore, options)\n\n    return {\n      cid,\n      path: file.path,\n      unixfs: f,\n      size: buffer.length + node.Links.reduce((acc, curr) => acc + curr.Tsize, 0)\n    }\n  }\n\n  return reducer\n}\n\n/**\n * @type {import('../../types').UnixFSV1DagBuilder<File>}\n */\nfunction fileBuilder (file, block, options) {\n  const dagBuilder = dagBuilders[options.strategy]\n\n  if (!dagBuilder) {\n    throw errCode(new Error(`Unknown importer build strategy name: ${options.strategy}`), 'ERR_BAD_STRATEGY')\n  }\n\n  return dagBuilder(buildFileBatch(file, block, options), reduce(file, block, options), options)\n}\n\nexport default fileBuilder\n"],"mappings":";;;;;;AAAA,OAAOA,OAAO,MAAM,UAAU;AAC9B,SAASC,MAAM,QAAQ,aAAa;AACpC,OAAOC,OAAO,MAAM,wBAAwB;AAC5C,SAASC,MAAM,EAAEC,OAAO,QAAQ,cAAc;AAC9C,OAAOC,aAAa,MAAM,mBAAmB;AAC7C,OAAO,KAAKC,QAAQ,MAAM,yBAAyB;AACnD,OAAO,KAAKC,KAAK,MAAM,cAAc;AAErC,OAAOC,OAAO,MAAM,WAAW;AAC/B,OAAOC,WAAW,MAAM,eAAe;AACvC,OAAOC,UAAU,MAAM,cAAc;AACrC,OAAOC,gBAAgB,MAAM,sBAAsB;;AAEnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA,IAAMC,WAAW,GAAG;EAClBC,IAAI,EAAEL,OAAO;EACbM,QAAQ,EAAEL,WAAW;EACrBM,OAAO,EAAEL;AACX,CAAC;;AAED;AACA;AACA;AACA;AACA;AAJA,SAKiBM,cAAcA,CAAAC,EAAA,EAAAC,GAAA,EAAAC,GAAA;EAAA,OAAAC,eAAA,CAAAC,KAAA,OAAAC,SAAA;AAAA;AA+B/B;AACA;AACA;AACA;AACA;AAJA,SAAAF,gBAAA;EAAAA,eAAA,GAAAG,mBAAA,eAAAC,mBAAA,GAAAC,IAAA,CA/BA,SAAAC,QAAiCC,IAAI,EAAEC,UAAU,EAAEC,OAAO;IAAA,IAAAC,KAAA,EAAAC,QAAA,EAAAC,cAAA,EAAAC,yBAAA,EAAAC,iBAAA,EAAAC,cAAA,EAAAC,SAAA,EAAAC,KAAA,EAAAC,KAAA;IAAA,OAAAd,mBAAA,GAAAe,IAAA,UAAAC,SAAAC,QAAA;MAAA,kBAAAA,QAAA,CAAAC,IAAA,GAAAD,QAAA,CAAAE,IAAA;QAAA;UACpDb,KAAK,GAAG,CAAC,CAAC;UAId,IAAI,OAAOD,OAAO,CAACG,cAAc,KAAK,UAAU,EAAE;YAChDA,cAAc,GAAGH,OAAO,CAACG,cAAc;UACzC,CAAC,MAAM;YACLA,cAAc,GAAGrB,gBAAgB;UACnC;UAACsB,yBAAA;UAAAC,iBAAA;UAAAO,QAAA,CAAAC,IAAA;UAAAN,SAAA,GAAAQ,cAAA,CAEyBvC,aAAa,CAAC2B,cAAc,CAACL,IAAI,EAAEC,UAAU,EAAEC,OAAO,CAAC,EAAEA,OAAO,CAACgB,qBAAqB,CAAC;QAAA;UAAAJ,QAAA,CAAAE,IAAA;UAAA,OAAAG,oBAAA,CAAAV,SAAA,CAAAO,IAAA;QAAA;UAAA,MAAAV,yBAAA,KAAAI,KAAA,GAAAI,QAAA,CAAAM,IAAA,EAAAC,IAAA;YAAAP,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAhGL,KAAK,GAAAD,KAAA,CAAAY,KAAA;UACpBnB,KAAK,EAAE;UAAA,MAEHA,KAAK,KAAK,CAAC;YAAAW,QAAA,CAAAE,IAAA;YAAA;UAAA;UACbZ,QAAQ,GAAGO,KAAK;UAAA,OAAAG,QAAA,CAAAS,MAAA;QAAA;UAAA,MAEPpB,KAAK,KAAK,CAAC,IAAIC,QAAQ;YAAAU,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAAE,IAAA;UAChC,OAAMZ,QAAQ;QAAA;UACdA,QAAQ,GAAG,IAAI;QAAA;UAAAU,QAAA,CAAAE,IAAA;UAGjB,OAAML,KAAK;QAAA;UAAAL,yBAAA;UAAAQ,QAAA,CAAAE,IAAA;UAAA;QAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA;QAAA;UAAAF,QAAA,CAAAC,IAAA;UAAAD,QAAA,CAAAU,EAAA,GAAAV,QAAA;UAAAP,iBAAA;UAAAC,cAAA,GAAAM,QAAA,CAAAU,EAAA;QAAA;UAAAV,QAAA,CAAAC,IAAA;UAAAD,QAAA,CAAAC,IAAA;UAAA,MAAAT,yBAAA,IAAAG,SAAA,CAAAgB,MAAA;YAAAX,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAAF,QAAA,CAAAE,IAAA;UAAA,OAAAG,oBAAA,CAAAV,SAAA,CAAAgB,MAAA;QAAA;UAAAX,QAAA,CAAAC,IAAA;UAAA,KAAAR,iBAAA;YAAAO,QAAA,CAAAE,IAAA;YAAA;UAAA;UAAA,MAAAR,cAAA;QAAA;UAAA,OAAAM,QAAA,CAAAY,MAAA;QAAA;UAAA,OAAAZ,QAAA,CAAAY,MAAA;QAAA;UAAA,KAGTtB,QAAQ;YAAAU,QAAA,CAAAE,IAAA;YAAA;UAAA;UACVZ,QAAQ,CAACuB,MAAM,GAAG,IAAI;UAAAb,QAAA,CAAAE,IAAA;UACtB,OAAMZ,QAAQ;QAAA;QAAA;UAAA,OAAAU,QAAA,CAAAc,IAAA;MAAA;IAAA,GAAA7B,OAAA;EAAA,CAEjB;EAAA,OAAAN,eAAA,CAAAC,KAAA,OAAAC,SAAA;AAAA;AAOD,IAAMkC,MAAM,GAAG,SAATA,MAAMA,CAAI7B,IAAI,EAAEC,UAAU,EAAEC,OAAO,EAAK;EAC5C;AACF;AACA;EAFE,SAGe4B,OAAOA,CAAAC,GAAA;IAAA,OAAAC,QAAA,CAAAtC,KAAA,OAAAC,SAAA;EAAA;EAAA,SAAAqC,SAAA;IAAAA,QAAA,GAAAC,iBAAA,eAAApC,mBAAA,GAAAC,IAAA,CAAtB,SAAAoC,SAAwBC,MAAM;MAAA,IAAAC,IAAA,EAAAC,OAAA,EAAAC,CAAA,EAAAC,KAAA,EAAAC,IAAA,EAAAC,MAAA,EAAAC,GAAA;MAAA,OAAA7C,mBAAA,GAAAe,IAAA,UAAA+B,UAAAC,SAAA;QAAA,kBAAAA,SAAA,CAAA7B,IAAA,GAAA6B,SAAA,CAAA5B,IAAA;UAAA;YAAA,MACxBmB,MAAM,CAACU,MAAM,KAAK,CAAC,IAAIV,MAAM,CAAC,CAAC,CAAC,CAACR,MAAM,IAAIzB,OAAO,CAAC4C,sBAAsB;cAAAF,SAAA,CAAA5B,IAAA;cAAA;YAAA;YACrEoB,IAAI,GAAGD,MAAM,CAAC,CAAC,CAAC;YAAA,MAElBnC,IAAI,CAAC+C,KAAK,KAAKC,SAAS,IAAIhD,IAAI,CAACiD,IAAI,KAAKD,SAAS;cAAAJ,SAAA,CAAA5B,IAAA;cAAA;YAAA;YAAA4B,SAAA,CAAA5B,IAAA;YAAA,OAGlCf,UAAU,CAACiD,GAAG,CAACd,IAAI,CAACM,GAAG,CAAC;UAAA;YAAvCD,OAAM,GAAAG,SAAA,CAAAxB,IAAA;YAEVgB,IAAI,CAACe,MAAM,GAAG,IAAI7E,MAAM,CAAC;cACvB8E,IAAI,EAAE,MAAM;cACZL,KAAK,EAAE/C,IAAI,CAAC+C,KAAK;cACjBE,IAAI,EAAEjD,IAAI,CAACiD,IAAI;cACfI,IAAI,EAAEZ;YACR,CAAC,CAAC;YAEFA,OAAM,GAAGjE,MAAM,CAACC,OAAO,CAAC;cAAE6E,IAAI,EAAElB,IAAI,CAACe,MAAM,CAACI,OAAO,CAAC;YAAE,CAAC,CAAC,CAAC;;YAEzD;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YACA;YAAAX,SAAA,CAAA5B,IAAA;YAAA,OACiBzC,OAAO,CAACkE,OAAM,EAAExC,UAAU,EAAAuD,aAAA,CAAAA,aAAA,KACtCtD,OAAO;cACVuD,KAAK,EAAE7E,KAAK;cACZ8E,MAAM,EAAExD,OAAO,CAACwD,MAAM;cACtBC,UAAU,EAAEzD,OAAO,CAACyD;YAAU,EAC/B,CAAC;UAAA;YALFvB,IAAI,CAACM,GAAG,GAAAE,SAAA,CAAAxB,IAAA;YAMRgB,IAAI,CAACwB,IAAI,GAAGnB,OAAM,CAACI,MAAM;UAAA;YAAA,OAAAD,SAAA,CAAArB,MAAA,WAGpB;cACLmB,GAAG,EAAEN,IAAI,CAACM,GAAG;cACbmB,IAAI,EAAE7D,IAAI,CAAC6D,IAAI;cACfV,MAAM,EAAEf,IAAI,CAACe,MAAM;cACnBS,IAAI,EAAExB,IAAI,CAACwB;YACb,CAAC;UAAA;YAGH;YACMtB,CAAC,GAAG,IAAIhE,MAAM,CAAC;cACnB8E,IAAI,EAAE,MAAM;cACZL,KAAK,EAAE/C,IAAI,CAAC+C,KAAK;cACjBE,IAAI,EAAEjD,IAAI,CAACiD;YACb,CAAC,CAAC;YAEIV,KAAK,GAAGJ,MAAM,CACjB2B,MAAM,CAAC,UAAA1B,IAAI,EAAI;cACd,IAAIA,IAAI,CAACM,GAAG,CAACqB,IAAI,KAAKpF,QAAQ,CAACoF,IAAI,IAAI3B,IAAI,CAACwB,IAAI,EAAE;gBAChD,OAAO,IAAI;cACb;cAEA,IAAIxB,IAAI,CAACe,MAAM,IAAI,CAACf,IAAI,CAACe,MAAM,CAACE,IAAI,IAAIjB,IAAI,CAACe,MAAM,CAACa,QAAQ,CAAC,CAAC,EAAE;gBAC9D,OAAO,IAAI;cACb;cAEA,OAAOC,OAAO,CAAC7B,IAAI,CAACe,MAAM,IAAIf,IAAI,CAACe,MAAM,CAACE,IAAI,IAAIjB,IAAI,CAACe,MAAM,CAACE,IAAI,CAACR,MAAM,CAAC;YAC5E,CAAC,CAAC,CACDqB,GAAG,CAAC,UAAC9B,IAAI,EAAK;cACb,IAAIA,IAAI,CAACM,GAAG,CAACqB,IAAI,KAAKpF,QAAQ,CAACoF,IAAI,EAAE;gBACnC;gBACAzB,CAAC,CAAC6B,YAAY,CAAC/B,IAAI,CAACwB,IAAI,CAAC;gBAEzB,OAAO;kBACLQ,IAAI,EAAE,EAAE;kBACRC,KAAK,EAAEjC,IAAI,CAACwB,IAAI;kBAChBU,IAAI,EAAElC,IAAI,CAACM;gBACb,CAAC;cACH;cAEA,IAAI,CAACN,IAAI,CAACe,MAAM,IAAI,CAACf,IAAI,CAACe,MAAM,CAACE,IAAI,EAAE;gBACrC;gBACAf,CAAC,CAAC6B,YAAY,CAAE/B,IAAI,CAACe,MAAM,IAAIf,IAAI,CAACe,MAAM,CAACa,QAAQ,CAAC,CAAC,IAAK,CAAC,CAAC;cAC9D,CAAC,MAAM;gBACL;gBACA1B,CAAC,CAAC6B,YAAY,CAAC/B,IAAI,CAACe,MAAM,CAACE,IAAI,CAACR,MAAM,CAAC;cACzC;cAEA,OAAO;gBACLuB,IAAI,EAAE,EAAE;gBACRC,KAAK,EAAEjC,IAAI,CAACwB,IAAI;gBAChBU,IAAI,EAAElC,IAAI,CAACM;cACb,CAAC;YACH,CAAC,CAAC;YAEEF,IAAI,GAAG;cACXc,IAAI,EAAEhB,CAAC,CAACiB,OAAO,CAAC,CAAC;cACjBgB,KAAK,EAAEhC;YACT,CAAC;YACKE,MAAM,GAAGjE,MAAM,CAACC,OAAO,CAAC+D,IAAI,CAAC,CAAC;YAAAI,SAAA,CAAA5B,IAAA;YAAA,OAClBzC,OAAO,CAACkE,MAAM,EAAExC,UAAU,EAAEC,OAAO,CAAC;UAAA;YAAhDwC,GAAG,GAAAE,SAAA,CAAAxB,IAAA;YAAA,OAAAwB,SAAA,CAAArB,MAAA,WAEF;cACLmB,GAAG,EAAHA,GAAG;cACHmB,IAAI,EAAE7D,IAAI,CAAC6D,IAAI;cACfV,MAAM,EAAEb,CAAC;cACTsB,IAAI,EAAEnB,MAAM,CAACI,MAAM,GAAGL,IAAI,CAAC+B,KAAK,CAAC1C,MAAM,CAAC,UAAC2C,GAAG,EAAEC,IAAI;gBAAA,OAAKD,GAAG,GAAGC,IAAI,CAACJ,KAAK;cAAA,GAAE,CAAC;YAC5E,CAAC;UAAA;UAAA;YAAA,OAAAzB,SAAA,CAAAhB,IAAA;QAAA;MAAA,GAAAM,QAAA;IAAA,CACF;IAAA,OAAAF,QAAA,CAAAtC,KAAA,OAAAC,SAAA;EAAA;EAED,OAAOmC,OAAO;AAChB,CAAC;;AAED;AACA;AACA;AACA,SAAS4C,WAAWA,CAAE1E,IAAI,EAAE2E,KAAK,EAAEzE,OAAO,EAAE;EAC1C,IAAM0E,UAAU,GAAG3F,WAAW,CAACiB,OAAO,CAAC2E,QAAQ,CAAC;EAEhD,IAAI,CAACD,UAAU,EAAE;IACf,MAAMvG,OAAO,CAAC,IAAIyG,KAAK,0CAAAC,MAAA,CAA0C7E,OAAO,CAAC2E,QAAQ,CAAE,CAAC,EAAE,kBAAkB,CAAC;EAC3G;EAEA,OAAOD,UAAU,CAACvF,cAAc,CAACW,IAAI,EAAE2E,KAAK,EAAEzE,OAAO,CAAC,EAAE2B,MAAM,CAAC7B,IAAI,EAAE2E,KAAK,EAAEzE,OAAO,CAAC,EAAEA,OAAO,CAAC;AAChG;AAEA,eAAewE,WAAW"},"metadata":{},"sourceType":"module","externalDependencies":[]}